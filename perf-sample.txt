                                HMBDC PERFORMANCE SAMPLE (updated with release 12-41)

transport           reliability/scalability          1-way latency measured on following hardware/OS
type                notes                            [a] Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz 8 core 16G RAM, CentOs8
                                                     [b] Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz, Ubuntu 18.04
                                                     [c] 1G CISCO SG200 switch (intel 82574L and 82579LM NIC)

qperf               industry standard TCP            0.1) 5us - 2ms for loopback messages (100B-10MB)
                    measurement tool                 0.2) 60us - 90ms for network messages (100B-10MB)
                    (use as REFERENCE only
                     , not part of hmbdc)

inter-thread        reliable,                        1.1) ~2us (shared poniter passing)
                    sender / receiver blocks         
                    if buffer full or no msg
                                                     

inter-process       reliable,                        2.1) less than 1us for 0cpy messages (100B-10MB)
(IPC)               sender blocks if buffer full     2.2) 1us - 4ms for regular messages (100B-10MB)
                    receivers could be purged
                    if too slow or crashed,
                    supports up to 256 
                    processes

tcpcast             as reliable/scalable as TCP,     3.1) 60us - 80ms for messages (100B-10MB)
                    slow receivers can be            
                    purged if configured at sender 
                    (see waitForSlowReceivers)

reliable UDP        as reliable as TCP               4.1) 60us - 80ms for messages (100B-10MB)
multicast           as scalable as UDP multicast,    
(rmcast)            slow receivers could be
                    dropped if configured at sender 
                    (see waitForSlowReceivers)

reliable netmap     as reliable as TCP               slightly better than rmcast
multicast           as scalable as UDP multicast     
(rnetmap)           slow receivers could be
                    dropped if configured at sender
                    (see waitForSlowReceivers)

--------------------------------------------------------------------------------------------------------------------
commands and notes when obtaining the above performance data
0.1)
[a] for s in 100 1000 10000 100000 1000000 10000000; do ./tips-ping-pong -r ping --cpuIndex 2 --msgPerSec 50 --netprot nonet --msgSize=$s --runTime 30 --use0cpy=0; done
0.2)
[b] qperf
[a] for s in 100 1000 10000 100000 1000000 10000000; do qperf -v -m $s 192.168.2.100  tcp_lat; done

1.1)
[a] for s in 100 1000 10000 100000 1000000 10000000; do ./tips-ping-pong -r both --cpuIndex 0 1 2 --msgPerSec 5000 --msgSize $s --runTime 10 --netprot nonet; done

2.1)
[b] ./tips-ping-pong -r pong --cpuIndex 4 --netprot nonet
[a] for s in 100 1000 10000 100000 1000000 10000000; do ./tips-ping-pong -r ping --cpuIndex 2 --msgPerSec 5000 --netprot nonet --msgSize=$s --runTime 30; done
2.2)
[b] ./tips-ping-pong -r pong --cpuIndex 4 --netprot nonet --use0cpy=0
[a] for s in 100 1000 10000 100000 1000000 10000000; do ./tips-ping-pong -r ping --cpuIndex 2 --msgPerSec 50 --netprot nonet --msgSize=$s --runTime 30 --use0cpy=0; done

3.1)
[b] ./tips-ping-pong -r pong --cpuIndex 4 -I 192.168.2.0/24
[a] for cfg in "100 500" "1000 500" "10000 500" "100000 500" "1000000 50" "10000000 10"; do a=($cfg); ./tips-ping-pong -r ping --cpuIndex 2 --msgPerSec ${a[1]} --netprot tcpcast --msgSize=${a[0]} --runTime 10 -I 192.168.2.0/24; done

4.1)
[b] ./tips-ping-pong -r pong --cpuIndex 4 -I 192.168.2.0/24 --netprot=rmcast 
[a] for cfg in "100 500" "1000 500" "10000 500" "100000 500" "1000000 50" "10000000 5"; do a=($cfg); ./tips-ping-pong -r ping --cpuIndex 2 --msgPerSec ${a[1]} --netprot rmcast --msgSize=${a[0]} --runTime 20 -I 192.168.2.0/24; done
